# Small Transformer Model Configuration
model_type: small_transformer
tokenizer_name: bert-base-uncased
num_labels: 2
max_length: 256

# Architecture parameters (optimized for ~4x faster training)
d_model: 192
n_heads: 4
n_layers: 3
ffn_dim: 768
dropout: 0.1
